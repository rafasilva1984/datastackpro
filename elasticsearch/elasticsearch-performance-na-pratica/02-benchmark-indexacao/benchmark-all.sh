#!/usr/bin/env bash
# benchmark-all.sh ‚Äî compara efeitos de refresh_interval e replicas na ingest√£o
# Requisitos: curl; (opcional) jq para m√©tricas mais leg√≠veis

set -euo pipefail

ES_URL="${ES_URL:-http://localhost:9200}"
INDEX="${INDEX:-infra-hosts}"
FILE="${FILE:-dados-10000.ndjson}"

# --- helpers ---------------------------------------------------------------

hr() { printf '%*s\n' "${COLUMNS:-80}" '' | tr ' ' -; }

wait_green() {
  # aguarda ES responder green/yellow/green
  for i in {1..60}; do
    if curl -fsS "$ES_URL/_cluster/health?pretty" >/dev/null; then return 0; fi
    sleep 1
  done
  echo "‚ö†Ô∏è  ES n√£o respondeu a tempo em $ES_URL"; exit 1
}

recreate_index() {
  curl -fsS -X DELETE "$ES_URL/$INDEX" >/dev/null || true
  curl -fsS -X PUT "$ES_URL/$INDEX" -H 'Content-Type: application/json' -d '{
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "refresh_interval": "1s"
    },
    "mappings": {
      "properties": {
        "host":      { "type": "keyword" },
        "servico":   { "type": "keyword" },
        "status":    { "type": "keyword" },
        "cpu":       { "type": "integer" },
        "memoria":   { "type": "integer" },
        "@timestamp":{ "type": "date" }
      }
    }
  }' >/dev/null
}

bulk_ingest() {
  local ndjson="$1"
  local start end
  start=$(date +%s)
  curl -fsS -H 'Content-Type: application/x-ndjson' -X POST "$ES_URL/_bulk" --data-binary "@${ndjson}" >/dev/null
  curl -fsS -X POST "$ES_URL/$INDEX/_refresh" >/dev/null
  end=$(date +%s)
  echo $((end - start))
}

docs_count() {
  curl -fsS "$ES_URL/$INDEX/_count" | grep -o '"count":[0-9]*' | cut -d: -f2
}

store_size_mb() {
  # pega size em MB do _cat/indices
  curl -fsS "$ES_URL/_cat/indices/$INDEX?bytes=mb" | awk '{print $(NF-1)}'
}

profile_latency_ms() {
  # mede lat√™ncia de uma query t√≠pica (warning + cpu>=85)
  # retorna soma aproximada de query_time dos shards (ms)
  local resp
  resp=$(curl -fsS -H 'Content-Type: application/json' -X POST "$ES_URL/$INDEX/_search" -d '{
    "size": 0,
    "profile": true,
    "query": {
      "bool": {
        "must": [
          { "match": { "status": "warning" } },
          { "range": { "cpu": { "gte": 85 } } }
        ]
      }
    }
  }')
  # extrai n√∫meros (sem jq), soma b√°sica
  echo "$resp" | tr ',{}[]' '\n' | grep -E '"time_in_nanos":[0-9]+' \
    | sed 's/[^0-9]//g' | awk '{sum+=$1} END{printf("%.2f", sum/1000000)}'
}

print_row() {
  # columns: Cen√°rio | Tempo(s) | Docs | Store(MB) | Profile(ms)
  printf "%-28s | %9s | %8s | %9s | %11s\n" "$1" "$2" "$3" "$4" "$5"
}

# --- prechecks -------------------------------------------------------------

if [[ ! -f "$FILE" ]]; then
  echo "‚ùå Arquivo NDJSON n√£o encontrado: $FILE"
  echo "   Esperado em 02-benchmark-indexacao/$FILE"
  exit 1
fi

echo "ES_URL=$ES_URL  INDEX=$INDEX  FILE=$FILE"
wait_green
hr

# --- CEN√ÅRIO 1: Base (refresh=1s, replicas=0) ------------------------------

echo "üîµ Cen√°rio 1: Base (refresh=1s, replicas=0)"
recreate_index
t1=$(bulk_ingest "$FILE")
c1=$(docs_count)
s1=$(store_size_mb)
p1=$(profile_latency_ms)
print_row "Base (1s, replicas=0)" "$t1" "$c1" "$s1" "$p1"
hr

# --- CEN√ÅRIO 2: refresh=-1 durante o bulk ---------------------------------

echo "üü† Cen√°rio 2: refresh=-1 (durante o bulk)"
recreate_index
curl -fsS -X PUT "$ES_URL/$INDEX/_settings" -H 'Content-Type: application/json' -d '{"index":{"refresh_interval":"-1"}}' >/dev/null
t2=$(bulk_ingest "$FILE")  # bulk com refresh desligado
# reativar
curl -fsS -X PUT "$ES_URL/$INDEX/_settings" -H 'Content-Type: application/json' -d '{"index":{"refresh_interval":"1s"}}' >/dev/null
c2=$(docs_count)
s2=$(store_size_mb)
p2=$(profile_latency_ms)
print_row "Bulk c/ refresh=-1" "$t2" "$c2" "$s2" "$p2"
hr

# --- CEN√ÅRIO 3: replicas=1 ap√≥s ingest√£o ----------------------------------

echo "üü£ Cen√°rio 3: replicas=1 (ap√≥s ingest√£o)"
recreate_index
t3=$(bulk_ingest "$FILE")
curl -fsS -X PUT "$ES_URL/$INDEX/_settings" -H 'Content-Type: application/json' -d '{"index":{"number_of_replicas":1}}' >/dev/null
# espera estabilizar r√©plicas
sleep 3
c3=$(docs_count)
s3=$(store_size_mb)
p3=$(profile_latency_ms)
print_row "Replicas=1 (p√≥s-bulk)" "$t3" "$c3" "$s3" "$p3"
hr

# --- RESUMO ---------------------------------------------------------------

echo "‚úÖ RESUMO (menor tempo = melhor para ingest√£o)"
printf "%-28s | %9s | %8s | %9s | %11s\n" "Cen√°rio" "Tempo(s)" "Docs" "Store(MB)" "Profile(ms)"
hr
print_row "Base (1s, replicas=0)" "$t1" "$c1" "$s1" "$p1"
print_row "Bulk c/ refresh=-1"   "$t2" "$c2" "$s2" "$p2"
print_row "Replicas=1 (p√≥s-bulk)" "$t3" "$c3" "$s3" "$p3"
hr

echo "üí° Interpreta√ß√£o:"
echo "- refresh=-1 costuma REDUZIR o tempo de ingest√£o (menos refresh/segmentos durante o bulk)."
echo "- replicas=1 tende a AUMENTAR custo de escrita; √∫til para leitura/HA, mas mais lento para ingest√£o."
echo "- Profile(ms) d√° uma no√ß√£o da soma de tempos de busca por shard (aprox.). Fa√ßa 2‚Äì3 rodadas e compare m√©dias."
